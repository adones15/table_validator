{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4471f880",
   "metadata": {},
   "source": [
    "## Instalação da biblioteca holidays (feriados) e import de todas as bibliotecas utilizadas no código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c070a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install holidays #--index-url http://artifactory.santanderbr.corp/artifactory/api/pypi/pypi-all/simple --trusted-host artifactory.santanderbr.corp\n",
    "#!pip install holidays\n",
    "import pandas as pd\n",
    "import requests, json, holidays\n",
    "from pyspark.sql.functions import to_timestamp, get_json_object, current_timestamp\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F, SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from funcoes import dados_sensiveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6aefbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionURL\n",
      "25/06/04 23:31:27 WARN Utils: Your hostname, adones-Nitro5 resolves to a loopback address: 127.0.1.1; using 192.168.15.35 instead (on interface wlp0s20f3)\n",
      "25/06/04 23:31:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/adones/Documentos/Python_Projects/table_validator/validator/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/adones/.ivy2/cache\n",
      "The jars for the packages stored in: /home/adones/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a5277702-80e0-48c0-b6c8-2e3150e19c81;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.2 in central\n",
      "\tfound io.delta#delta-storage;3.3.2 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 210ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.2 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.2 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a5277702-80e0-48c0-b6c8-2e3150e19c81\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/8ms)\n",
      "25/06/04 23:31:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Table Validator\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"./spark-warehouse\") \\\n",
    "    .config(\"javax.jdo.option.ConnectionURL\", \"jdbc:derby:./metastore_db;create=true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e87e9f",
   "metadata": {},
   "source": [
    "## Criando as variáveis de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adc8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt_hj = datetime.today()\n",
    "d_sem = dt_hj.weekday()\n",
    "dt_hj_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "exec_datetime = datetime.now()\n",
    "exec_timestamp_str = exec_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "ano_mes_corr = exec_datetime.strftime('%Y-%m')\n",
    "dia_20 = f\"{ano_mes_corr}-20\"\n",
    "dt_inic_mes = f\"{ano_mes_corr}-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf175f",
   "metadata": {},
   "source": [
    "## Função para obter qual o dia útil do mês estamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a6f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feriados_br = holidays.Brazil()\n",
    "\n",
    "def business_days(start_date, end_date):\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    num_business_days = 0\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        if current_date.weekday() < 5 and current_date not in feriados_br:\n",
    "            num_business_days += 1\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    if end_date.weekday() < 5 and end_date in feriados_br:\n",
    "        num_business_days += 1\n",
    "\n",
    "    return num_business_days\n",
    "\n",
    "dia_util = business_days(dt_inic_mes, dt_hj_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718018f",
   "metadata": {},
   "source": [
    "## Obtendo todas as tabelas automaticamente pelos schemas \n",
    "(Isso faz com que você faça a validação de todas as tabelas disponiveis nos schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4617f144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 23:31:37 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/06/04 23:31:37 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/06/04 23:31:40 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/06/04 23:31:40 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore adones@127.0.1.1\n",
      "25/06/04 23:31:40 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "[Stage 0:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+-----------+\n",
      "|namespace          |tableName               |isTemporary|\n",
      "+-------------------+------------------------+-----------+\n",
      "|s_customer_db      |abandoned_carts         |false      |\n",
      "|s_analytics_db     |ad_clicks               |false      |\n",
      "|s_analytics_db     |conversion_rates        |false      |\n",
      "|s_financial_risk_db|credit_exposure         |false      |\n",
      "|s_sales_data_db    |customer_returns        |false      |\n",
      "|s_customer_db      |feedback_surveys        |false      |\n",
      "|s_financial_risk_db|interest_rate_scenarios |false      |\n",
      "|s_analytics_db     |marketing_campaigns     |false      |\n",
      "|s_customer_db      |newsletter_subscriptions|false      |\n",
      "|s_sales_data_db    |order_items             |false      |\n",
      "|s_sales_data_db    |orders                  |false      |\n",
      "|s_sales_data_db    |payment_receipts        |false      |\n",
      "|s_customer_db      |purchase_funnels        |false      |\n",
      "|s_financial_risk_db|risk_limits             |false      |\n",
      "|s_sales_data_db    |sales_targets           |false      |\n",
      "|s_analytics_db     |session_durations       |false      |\n",
      "|s_financial_risk_db|stress_test_results     |false      |\n",
      "|s_customer_db      |user_sessions           |false      |\n",
      "|s_analytics_db     |website_traffic_logs    |false      |\n",
      "+-------------------+------------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lista_schemas = [\"s_analytics_db\", \"s_sales_data_db\", \"s_financial_risk_db\", \"s_customer_db\"] #Coloque os schemas das tabelas que deseja validar\n",
    "\n",
    "df_schemas = None\n",
    "for i in lista_schemas:\n",
    "    df_sch = spark.sql(f\"SHOW TABLES IN {i}\")\n",
    "    if df_schemas is None:\n",
    "        df_schemas = df_sch\n",
    "    else:\n",
    "        df_schemas = df_schemas.union(df_sch).dropDuplicates([\"tableName\"])\n",
    "df_schemas.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415197ac",
   "metadata": {},
   "source": [
    "### Selecionando apenas as tabelas que precisam ser verificadas no dia\n",
    "Aqui conseguimos retirar da validação do dia aquelas tabelas que precisam ser validadas apenas em dias específicosx por exemplo tabelas que são populadas apenas uma vez por mês ou uma vez por semana, ou tabelas que não são populadas no final de semana etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39af98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|nome_tabela                                |\n",
      "+-------------------------------------------+\n",
      "|s_analytics_db.ad_clicks                   |\n",
      "|s_analytics_db.conversion_rates            |\n",
      "|s_financial_risk_db.credit_exposure        |\n",
      "|s_sales_data_db.customer_returns           |\n",
      "|s_customer_db.feedback_surveys             |\n",
      "|s_financial_risk_db.interest_rate_scenarios|\n",
      "|s_customer_db.newsletter_subscriptions     |\n",
      "|s_sales_data_db.order_items                |\n",
      "|s_sales_data_db.orders                     |\n",
      "|s_sales_data_db.payment_receipts           |\n",
      "|s_customer_db.purchase_funnels             |\n",
      "|s_financial_risk_db.risk_limits            |\n",
      "|s_sales_data_db.sales_targets              |\n",
      "|s_analytics_db.session_durations           |\n",
      "|s_financial_risk_db.stress_test_results    |\n",
      "|s_analytics_db.website_traffic_logs        |\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tabelas = df_schemas \\\n",
    "    .withColumn(\"nome_tabela\", \n",
    "                F.concat(\n",
    "                    F.col(\"namespace\"), \n",
    "                    F.lit(\".\"), \n",
    "                    F.col(\"tableName\"))) \\\n",
    "    .withColumn(\"exec_hj\", #Criamos uma coluna para saber as tabelas que devem ser populadas hoje, a coluna terá apenas \"SIM\" ou \"NÃO\" para cada tabela\n",
    "        F.when(\n",
    "            F.col(\"nome_tabela\").isin(\"s_customer_db.abandoned_carts \", \"s_customer_db.abandoned_carts\") & #coloque aqui as tabelas que só rodam de sábado e domingo\n",
    "            (~F.lit(d_sem).isin(5, 6)), F.lit(\"Não\")\n",
    "        ).when(\n",
    "            F.col(\"nome_tabela\").isin(\"s_sales_data_db.invoices\", \"s_analytics_db.marketing_campaigns\") & #coloque aqui as tabelas que só rodam de sexta-feira\n",
    "            (~F.lit(d_sem).isin(4)), F.lit(\"Não\")\n",
    "        ).when(\n",
    "            F.col(\"nome_tabela\").isin(\"s_financial_risk_db.market_positions\") & #coloque aqui as tabelas que só rodam todo dia 20 de cada mês\n",
    "            (F.lit(dt_hj_str) != F.lit(dia_20)), F.lit(\"Não\") \\\n",
    "        ).when(\n",
    "            F.col(\"nome_tabela\").isin(\"s_financial_risk_db.risk_limits\") & #coloque aqui as tabelas que NÃO rodam de sábado e domingo\n",
    "            (F.lit(d_sem).isin(5, 6)), F.lit(\"Não\") \\\n",
    "        ).when(\n",
    "            F.col(\"nome_tabela\").isin(\"s_financial_risk_db.market_positions\") & #tabelas que só rodam no 4º dia útil\n",
    "            (~F.lit(dia_util).isin(4)), F.lit(\"Não\")\n",
    "        #Aqui você pode configurar da forma que precisar, coloquei alguns exemplos porém fica à critério de cada um\n",
    "        ).otherwise(F.lit(\"Sim\"))) \\\n",
    "    .filter(\n",
    "        (~F.col(\"nome_tabela\").isin(\"s_customer_db.user_sessions\", \"s_financial_risk_db.liquidity_reports\")) & #tabelas que não quer fazer a validação por algum motivo, coloque aqui para retirar do validador\n",
    "        (F.col(\"exec_hj\").isin(\"Sim\"))) \\\n",
    "    .select(\"nome_tabela\") #fazemos o filtro acima para no final o dataframe ficar apenas com o nome das tabelas que precisam ser validadas no dia da execução\n",
    "    \n",
    "df_tabelas.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034fdcb7",
   "metadata": {},
   "source": [
    "### Aqui é feito um teste para ver se as tabelas tem permissão de acesso.\n",
    "Se tiver alguma tabela que não possui acesso ela será retirada do validador para não quebrar o código e será colocada em uma lista de tabelas sem permissão e no final vai na mensagem informando que não tem acesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf2e642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "tab_sem_per = []\n",
    "for tb_name in df_tabelas.collect():\n",
    "    try:\n",
    "        spark.sql(f\"select * from {tb_name['nome_tabela']} limit 1\")\n",
    "    except:\n",
    "        tab_sem_per.append(tb_name['nome_tabela'])\n",
    "df_tabelas = df_tabelas.filter(~F.col(\"nome_tabela\").isin(tab_sem_per))\n",
    "print(tab_sem_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6a295",
   "metadata": {},
   "source": [
    "### Verificando se o job executou ou não\n",
    "Aqui coletamos os dados da tabela através do \"describe history\" pegando a ultima atualização e trazendo dados como data e horário da execução, quantidade de linhas inseridas, nome da tabela e do job.\n",
    "Aqui é feito um loop no dataframe de tabelas para pegar os dados de todas as tabelas que estão para ser validadas no dia da execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7239f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+----------+----------+--------+--------+\n",
      "|nome_tabela_Desc                           |qtd_linhas|data_exec |hr_exec |job_name|\n",
      "+-------------------------------------------+----------+----------+--------+--------+\n",
      "|s_analytics_db.ad_clicks                   |194       |2025-06-04|20:30:21|NULL    |\n",
      "|s_analytics_db.conversion_rates            |186       |2025-06-04|20:30:23|NULL    |\n",
      "|s_financial_risk_db.credit_exposure        |178       |2025-06-04|20:30:25|NULL    |\n",
      "|s_sales_data_db.customer_returns           |170       |2025-06-04|20:30:27|NULL    |\n",
      "|s_customer_db.feedback_surveys             |162       |2025-06-04|20:30:29|NULL    |\n",
      "|s_financial_risk_db.interest_rate_scenarios|154       |2025-06-04|20:30:30|NULL    |\n",
      "|s_customer_db.newsletter_subscriptions     |138       |2025-06-04|20:30:34|NULL    |\n",
      "|s_sales_data_db.order_items                |130       |2025-06-04|20:30:35|NULL    |\n",
      "|s_sales_data_db.orders                     |122       |2025-06-04|20:30:37|NULL    |\n",
      "|s_sales_data_db.payment_receipts           |114       |2025-06-04|20:30:39|NULL    |\n",
      "|s_customer_db.purchase_funnels             |106       |2025-06-04|20:30:40|NULL    |\n",
      "|s_financial_risk_db.risk_limits            |98        |2025-06-04|20:30:42|NULL    |\n",
      "|s_sales_data_db.sales_targets              |90        |2025-06-04|20:30:43|NULL    |\n",
      "|s_analytics_db.session_durations           |82        |2025-06-04|20:30:45|NULL    |\n",
      "|s_financial_risk_db.stress_test_results    |74        |2025-06-04|20:30:46|NULL    |\n",
      "|s_analytics_db.website_traffic_logs        |58        |2025-06-04|20:30:49|NULL    |\n",
      "+-------------------------------------------+----------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = None\n",
    "for table_name in df_tabelas.collect():\n",
    "    history_df = spark.sql(f\"\"\"DESCRIBE HISTORY {table_name['nome_tabela']}\"\"\") \\\n",
    "        .filter( #Pegando apenas escrita de tabela e desconsiderando outras operações\n",
    "            (~F.col(\"operation\").isin(\"VACUUM START\", \"VACUUM END\", \"MERGE\"))) \\\n",
    "        .withColumn(\"data_exec\", \n",
    "                    F.to_date(F.col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "        .withColumn(\"hr_exec\", \n",
    "                    F.date_format(\n",
    "                        (F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\") \n",
    "                        - F.expr(\"INTERVAL 3 HOURS\")), \"HH:mm:ss\")) \\\n",
    "        .withColumn(\"nome_tabela_Desc\", \n",
    "                    F.lit(table_name[\"nome_tabela\"]))\n",
    "\n",
    "    # Obtendo apenas a última execução de cada tabela\n",
    "    recent_update_df = history_df.orderBy(F.col(\"timestamp\").desc()).limit(1)\n",
    "\n",
    "    recent_update_df = recent_update_df \\\n",
    "        .withColumn(\"operationMetricsStr\", \n",
    "                    F.to_json(F.col(\"operationMetrics\"))) \\\n",
    "        .withColumn(\"job\", F.to_json(F.col(\"job\")))\n",
    "\n",
    "    #obtendo o nome do job e a quantidade de linhas inseridas\n",
    "    recent_update_df = recent_update_df \\\n",
    "        .withColumn(\"qtd_linhas\", \n",
    "                    F.get_json_object(F.col(\"operationMetricsStr\"), \"$.numOutputRows\").cast(IntegerType())) \\\n",
    "        .withColumn(\"job_name\",\n",
    "                    F.get_json_object(F.col(\"job\"), \"$.jobName\"))\n",
    "\n",
    "    recent_update_df = recent_update_df \\\n",
    "        .select(\n",
    "            \"nome_tabela_Desc\",\n",
    "            \"qtd_linhas\",\n",
    "            \"data_exec\",\n",
    "            \"hr_exec\",            \n",
    "            \"job_name\"\n",
    "        )\n",
    "\n",
    "    if df_final is None: # Concatenando os dados no DataFrame final\n",
    "        df_final = recent_update_df\n",
    "    else:\n",
    "        df_final = df_final.union(recent_update_df)\n",
    "\n",
    "df_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60982f7",
   "metadata": {},
   "source": [
    "### Aplicando status final\n",
    "Aqui é criada a coluna \"status\" que vai mostrar se o job deu \"Sucesso\", \"Falha\" ou \"Executou hoje porém não escreveu dado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e5d8549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+--------+----------+----------+--------+-------+----------+\n",
      "|nome_tabela                                |job_name|qtd_linhas|data_exec |hr_exec |status |DT_REFE   |\n",
      "+-------------------------------------------+--------+----------+----------+--------+-------+----------+\n",
      "|s_analytics_db.ad_clicks                   |NULL    |194       |2025-06-04|20:30:21|Sucesso|2025-06-04|\n",
      "|s_analytics_db.conversion_rates            |NULL    |186       |2025-06-04|20:30:23|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.credit_exposure        |NULL    |178       |2025-06-04|20:30:25|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.customer_returns           |NULL    |170       |2025-06-04|20:30:27|Sucesso|2025-06-04|\n",
      "|s_customer_db.feedback_surveys             |NULL    |162       |2025-06-04|20:30:29|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.interest_rate_scenarios|NULL    |154       |2025-06-04|20:30:30|Sucesso|2025-06-04|\n",
      "|s_customer_db.newsletter_subscriptions     |NULL    |138       |2025-06-04|20:30:34|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.order_items                |NULL    |130       |2025-06-04|20:30:35|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.orders                     |NULL    |122       |2025-06-04|20:30:37|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.payment_receipts           |NULL    |114       |2025-06-04|20:30:39|Sucesso|2025-06-04|\n",
      "|s_customer_db.purchase_funnels             |NULL    |106       |2025-06-04|20:30:40|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.risk_limits            |NULL    |98        |2025-06-04|20:30:42|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.sales_targets              |NULL    |90        |2025-06-04|20:30:43|Sucesso|2025-06-04|\n",
      "|s_analytics_db.session_durations           |NULL    |82        |2025-06-04|20:30:45|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.stress_test_results    |NULL    |74        |2025-06-04|20:30:46|Sucesso|2025-06-04|\n",
      "|s_analytics_db.website_traffic_logs        |NULL    |58        |2025-06-04|20:30:49|Sucesso|2025-06-04|\n",
      "+-------------------------------------------+--------+----------+----------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_final = df_tabelas.alias(\"a\") \\\n",
    "    .join(df_final.alias(\"b\"), \n",
    "          on=F.col(\"a.nome_tabela\") == F.col(\"b.nome_tabela_Desc\"), \n",
    "          how=\"left\") \\\n",
    "    .withColumn(\"status\", \n",
    "        F.when((F.col(\"qtd_linhas\") > 0) & (F.col(\"data_exec\") == dt_hj_str), \"Sucesso\") \\\n",
    "        .otherwise(F.lit(\"Falha\"))) \\\n",
    "    .withColumn(\"status\", \n",
    "        F.when(\n",
    "            (F.col(\"qtd_linhas\").isNull()) | (F.col(\"qtd_linhas\") == 0) & \n",
    "            (F.col(\"data_exec\") == dt_hj_str), \"Executou hoje porém não escreveu dado\") \\\n",
    "        .otherwise(F.col(\"status\"))) \\\n",
    "    .withColumn(\"DT_REFE\",\n",
    "        F.lit(dt_hj_str)) \\\n",
    "    .select(\n",
    "        \"nome_tabela\",\n",
    "        \"job_name\",\n",
    "        \"qtd_linhas\",\n",
    "        \"data_exec\",\n",
    "        \"hr_exec\",      \n",
    "        \"status\",\n",
    "        \"DT_REFE\"\n",
    "    )\n",
    "\n",
    "join_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d65e7",
   "metadata": {},
   "source": [
    "### Obtendo o horário atual e filtrando o dataframe final\n",
    "Como o job roda de 1 em 1 hora, fiz essa validação para ele pegar apenas os job's que rodam da hora que está sendo executado para baixo para não ter perigo de aplicar \"Falha\" a um job que só roda algum tempo depois do horário que está sendo feita a validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3cb66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_hour = str(spark.sql(\"SELECT hour(current_timestamp()) as current_hour\").collect()[0][0] - 3).zfill(2)\n",
    "hr_1 = f\"{current_hour}:00:00\"\n",
    "join_final = join_final.filter(\n",
    "    F.to_timestamp(\"hr_exec\", \"HH:mm:ss\") <= F.to_timestamp(F.lit(hr_1), \"HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a636d9",
   "metadata": {},
   "source": [
    "### Criando o dataframe de falhas\n",
    "Aqui filtramos o dataframe final para ter apenas \"Falha\" ou \"Executou hoje porém não escreveu dado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d289e45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+--------+----------+----------+--------+-------+----------+\n",
      "|nome_tabela                                |job_name|qtd_linhas|data_exec |hr_exec |status |DT_REFE   |\n",
      "+-------------------------------------------+--------+----------+----------+--------+-------+----------+\n",
      "|s_analytics_db.ad_clicks                   |NULL    |194       |2025-06-04|20:30:21|Sucesso|2025-06-04|\n",
      "|s_analytics_db.conversion_rates            |NULL    |186       |2025-06-04|20:30:23|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.credit_exposure        |NULL    |178       |2025-06-04|20:30:25|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.customer_returns           |NULL    |170       |2025-06-04|20:30:27|Sucesso|2025-06-04|\n",
      "|s_customer_db.feedback_surveys             |NULL    |162       |2025-06-04|20:30:29|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.interest_rate_scenarios|NULL    |154       |2025-06-04|20:30:30|Sucesso|2025-06-04|\n",
      "|s_customer_db.newsletter_subscriptions     |NULL    |138       |2025-06-04|20:30:34|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.order_items                |NULL    |130       |2025-06-04|20:30:35|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.orders                     |NULL    |122       |2025-06-04|20:30:37|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.payment_receipts           |NULL    |114       |2025-06-04|20:30:39|Sucesso|2025-06-04|\n",
      "|s_customer_db.purchase_funnels             |NULL    |106       |2025-06-04|20:30:40|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.risk_limits            |NULL    |98        |2025-06-04|20:30:42|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.sales_targets              |NULL    |90        |2025-06-04|20:30:43|Sucesso|2025-06-04|\n",
      "|s_analytics_db.session_durations           |NULL    |82        |2025-06-04|20:30:45|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.stress_test_results    |NULL    |74        |2025-06-04|20:30:46|Sucesso|2025-06-04|\n",
      "|s_analytics_db.website_traffic_logs        |NULL    |58        |2025-06-04|20:30:49|Sucesso|2025-06-04|\n",
      "+-------------------------------------------+--------+----------+----------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_falhas = join_final#.filter((F.col(\"status\") == \"Falha\") | (F.col(\"status\") == \"Executou hoje porém não escreveu dado\"))\n",
    "df_falhas.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ff77c",
   "metadata": {},
   "source": [
    "### Envio da notificação pelo Teams\n",
    "Aqui é feita a verificação se o df_falhas tiver alguma linha enviamos a notificação para o Teams com o dataframe de falhas para que possa ser feita uma ação o quanto antes, caso contrário a notificação informará que não teve falha em job's no horário da verificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64762b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adones/Documentos/Python_Projects/table_validator/validator/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'alunoimpacta.webhook.office.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensagem enviada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "if df_falhas.count() >= 1:\n",
    "    webhook_url = dados_sensiveis.wb_str()\n",
    "    tb_dash_html = df_falhas.toPandas().to_html(index=False) #transformando o dataframe pyspark em html pra enviar via teams\n",
    "    if len(tab_sem_per) > 0:\n",
    "        message = {\n",
    "            \"title\": f\"FALHA DE JOB DAS {current_hour}:00 - Timestamp validado: {exec_timestamp_str}\",\n",
    "            \"text\": f\"ATENÇÃO! ALGUM JOB EXECUTOU ATÉ AS (current_hour):59:00 E EXECUTOU COM FALHA OU NÃO ESCREVEU DADO NA DATA DE HOJE ({dt_hj_str})***<br>\"\n",
    "                    f\"{tb_dash_html}<br>\"\n",
    "                    f\"Essas tabelas sem permissão de acesso conforme abaixo:<br>\"\n",
    "                    f\"{tab_sem_per}<br>\"\n",
    "                    f\"FAVOR PEDIR PARA LIBERAR O ACESSO AS TABELAS<br>\"\n",
    "        }\n",
    "    else:\n",
    "        message = {\n",
    "            \"title\": f\"FALHA DE JOB - Timestamp validado: {exec_timestamp_str}\",\n",
    "            \"text\": f\"ATENÇÃO! ALGUM JOB EXECUTOU ATÉ AS {current_hour}:59:00 E EXECUTOU COM FALHA OU NÃO ESCREVEU DADO NA DATA DE HOJE ({dt_hj_str})***<br>\"\n",
    "                    f\"{tb_dash_html}<br>\"\n",
    "        }\n",
    "\n",
    "    #proxies = {\"http\": \"coloque aqui seu proxy\", \"https\": \"coloque aqui seu proxy\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            webhook_url,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            data=json.dumps(message),\n",
    "            timeout=30,\n",
    "            verify=False,\n",
    "            #proxies=proxies\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\"Mensagem enviada com sucesso!\")\n",
    "        else:\n",
    "            print(f\"Falha ao enviar mensagem. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao enviar mensagem: {e}\")\n",
    "\n",
    "else:\n",
    "    webhook_url = dados_sensiveis.wb_str()\n",
    "\n",
    "    tb_dash_html = df_falhas.toPandas().to_html(index=False)\n",
    "    if len(tab_sem_per) > 0:\n",
    "        message = {\n",
    "            \"title\": f\"VALIDAÇÃO DE JOB'S DAS {current_hour}:00 - Timestamp validado: {exec_timestamp_str}\",\n",
    "            \"text\": f\"*****ATENÇÃO TODOS OS JOB'S QUE RODAM ATÉ AS {current_hour}:59:00 EXECUTARAM COM SUCESSO!!!*****<br>\"\n",
    "                    f\"<br>Existem tabelas sem permissão de acesso conforme abaixo:<br>\"\n",
    "                    f\"{tab_sem_per}<br>\"\n",
    "                    f\"<br>FAVOR PEDIR PARA LIBERAR O ACESSO AS TABELAS<br>*****\"\n",
    "        }\n",
    "    else:\n",
    "        message = {\n",
    "            \"title\": f\"VALIDAÇÃO DE JOB'S DAS {current_hour}:00 - Timestamp validado: {exec_timestamp_str}\",\n",
    "            \"text\": f\"*****ATENÇÃO TODOS OS JOB'S QUE RODAM ATÉ AS {current_hour}:59:00 EXECUTARAM COM SUCESSO!!!*****<br>\"\n",
    "        }\n",
    "\n",
    "    #proxies = {\"http\": \"coloque aqui seu proxy\", \"https\": \"coloque aqui seu proxy\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            webhook_url,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            data=json.dumps(message),\n",
    "            timeout=30,\n",
    "            verify=False,\n",
    "            #p#oxies=proxies\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\"Mensagem enviada com sucesso!\")\n",
    "        else:\n",
    "            print(f\"Falha ao enviar mensagem. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao enviar mensagem: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee8fb4",
   "metadata": {},
   "source": [
    "### Salvando o log na tabela de validação para caso necessite de histórico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec58ceaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 23:37:00 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/06/04 23:37:02 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`sandbox`.`validador_jobs` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/06/04 23:37:02 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/06/04 23:37:03 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/06/04 23:37:03 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/06/04 23:37:03 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "#colocar aqui para popular a tabela de log\n",
    "\n",
    "join_final.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"DT_REFE\") \\\n",
    "    .saveAsTable(\"sandbox.validador_jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b6a9320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+--------+----------+----------+--------+-------+----------+\n",
      "|nome_tabela                                |job_name|qtd_linhas|data_exec |hr_exec |status |DT_REFE   |\n",
      "+-------------------------------------------+--------+----------+----------+--------+-------+----------+\n",
      "|s_analytics_db.ad_clicks                   |NULL    |194       |2025-06-04|20:30:21|Sucesso|2025-06-04|\n",
      "|s_analytics_db.conversion_rates            |NULL    |186       |2025-06-04|20:30:23|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.credit_exposure        |NULL    |178       |2025-06-04|20:30:25|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.customer_returns           |NULL    |170       |2025-06-04|20:30:27|Sucesso|2025-06-04|\n",
      "|s_customer_db.feedback_surveys             |NULL    |162       |2025-06-04|20:30:29|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.interest_rate_scenarios|NULL    |154       |2025-06-04|20:30:30|Sucesso|2025-06-04|\n",
      "|s_customer_db.newsletter_subscriptions     |NULL    |138       |2025-06-04|20:30:34|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.order_items                |NULL    |130       |2025-06-04|20:30:35|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.orders                     |NULL    |122       |2025-06-04|20:30:37|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.payment_receipts           |NULL    |114       |2025-06-04|20:30:39|Sucesso|2025-06-04|\n",
      "|s_customer_db.purchase_funnels             |NULL    |106       |2025-06-04|20:30:40|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.risk_limits            |NULL    |98        |2025-06-04|20:30:42|Sucesso|2025-06-04|\n",
      "|s_sales_data_db.sales_targets              |NULL    |90        |2025-06-04|20:30:43|Sucesso|2025-06-04|\n",
      "|s_analytics_db.session_durations           |NULL    |82        |2025-06-04|20:30:45|Sucesso|2025-06-04|\n",
      "|s_financial_risk_db.stress_test_results    |NULL    |74        |2025-06-04|20:30:46|Sucesso|2025-06-04|\n",
      "|s_analytics_db.website_traffic_logs        |NULL    |58        |2025-06-04|20:30:49|Sucesso|2025-06-04|\n",
      "+-------------------------------------------+--------+----------+----------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from sandbox.validador_jobs\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
