{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4471f880",
   "metadata": {},
   "source": [
    "## Instalação da biblioteca holidays (feriados) e import de todas as bibliotecas utilizadas no código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c070a5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mholidays\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_timestamp, get_json_object, current_timestamp\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntegerType\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F, SparkSession\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "\n",
    "#!pip install holidays #--index-url http://artifactory.santanderbr.corp/artifactory/api/pypi/pypi-all/simple --trusted-host artifactory.santanderbr.corp\n",
    "#!pip install holidays\n",
    "import pandas as pd\n",
    "import requests, json, holidays\n",
    "from pyspark.sql.functions import to_timestamp, get_json_object, current_timestamp\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F, SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ValidadorTabelas\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e87e9f",
   "metadata": {},
   "source": [
    "## Criando as variáveis de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3adc8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt_hj = datetime.today()\n",
    "d_sem = dt_hj.weekday()\n",
    "dt_hj_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "exec_datetime = datetime.now()\n",
    "exec_timestamp_str = exec_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "ano_mes_corr = exec_datetime.strftime('%Y-%m')\n",
    "dia_20 = f\"{ano_mes_corr}-20\"\n",
    "dt_inic_mes = f\"{ano_mes_corr}-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf175f",
   "metadata": {},
   "source": [
    "## Função para obter qual o dia útil do mês estamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a6f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feriados_br = holidays.Brazil()\n",
    "\n",
    "def business_days(start_date, end_date):\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    num_business_days = 0\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        if current_date.weekday() < 5 and current_date not in feriados_br:\n",
    "            num_business_days += 1\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    if end_date.weekday() < 5 and end_date in feriados_br:\n",
    "        num_business_days += 1\n",
    "\n",
    "    return num_business_days\n",
    "\n",
    "dia_util = business_days(dt_inic_mes, dt_hj_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718018f",
   "metadata": {},
   "source": [
    "## Obtendo todas as tabelas automaticamente pelos schemas \n",
    "(Isso faz com que você faça a validação de todas as tabelas disponiveis nos schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_schemas = [\"schema_1\", \"schema_2\", \"schema_3\"] #Coloque os schemas das tabelas que deseja validar\n",
    "\n",
    "df_schemas = None\n",
    "for i in lista_schemas:\n",
    "    df_sch = spark.sql(f\"SHOW TABLES IN {i}\")\n",
    "    if df_schemas is None:\n",
    "        df_schemas = df_sch\n",
    "    else:\n",
    "        df_schemas = df_schemas.union(df_sch).dropDuplicates([\"tableName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415197ac",
   "metadata": {},
   "source": [
    "### Selecionando apenas as tabelas que precisam ser verificadas no dia\n",
    "Aqui conseguimos retirar da validação do dia aquelas tabelas que precisam ser validadas apenas em dias específicosx por exemplo tabelas que são populadas apenas uma vez por mês ou uma vez por semana, ou tabelas que não são populadas no final de semana etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39af98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tabelas = df_schemas \\\n",
    "    .withColumn(\"nome_tabela\", \n",
    "                F.concat(\n",
    "                    F.col(\"database\"), \n",
    "                    F.lit(\".\"), \n",
    "                    F.col(\"tableName\"))\n",
    "    ) \\\n",
    "    .withColumn(\"exec_hj\", #Criamos uma coluna para saber as tabelas que devem ser populadas hoje, a coluna terá apenas \"SIM\" ou \"NÃO\" para cada tabela\n",
    "        F.when(\n",
    "            F.col(\"nome_tabela\").isin(\"tabela_1\", \"tabela_2\", \"tabela_3\") & #coloque aqui as tabelas que só rodam de sábado e domingo\n",
    "            (~F.lit(d_sem).isin(5, 6)), F.lit(\"Não\")\n",
    "        ).when(\n",
    "            F.col(\"nome_tabela\").isin(\"tabela_1\", \"tabela_2\", \"tabela_3\") & #coloque aqui as tabelas que só rodam de sexta-feira\n",
    "            (~F.lit(d_sem).isin(4)), F.lit(\"Não\")\n",
    "        ).when(\n",
    "            F.col(\"nome_tabela\").isin(\"tabela_1\", \"tabela_2\", \"tabela_3\") & #coloque aqui as tabelas que só rodam todo dia 20 de cada mês\n",
    "            (~F.lit(dt_hj_str) == F.lit(dia_20)), F.lit(\"Não\")\n",
    "        ).when(\n",
    "            F.col(\"nome_tabela\").isin(\"tabela_1\", \"tabela_2\", \"tabela_3\") & #coloque aqui as tabelas que NÃO rodam de sábado e domingo\n",
    "            (F.lit(d_sem).isin(5, 6)), F.lit(\"Não\")\n",
    "        ).when(\n",
    "            F.col(\"nome_tabela\").isin(\"tabela_1\", \"tabela_2\", \"tabela_3\") & #tabelas que só rodam no 4º dia útil\n",
    "            (~F.lit(dia_util).isin(4)), F.lit(\"Não\")\n",
    "        #Aqui você pode configurar da forma que precisar, coloquei alguns exemplos porém fica à critério de cada um\n",
    "        ).otherwise(F.lit(\"Sim\"))) \\\n",
    "    .filter(\n",
    "        (~F.col(\"nome_tabela\").isin(\"tabela_1\", \"tabela_2\", \"tabela_3\")) & #tabelas que não quer fazer a validação por algum motivo, coloque aqui para retirar do validador\n",
    "        (F.col(\"exec_hj\").isin(\"Sim\"))) \\\n",
    "    .select(\"nome_tabela\") #fazemos o filtro acima para no final o dataframe ficar apenas com o nome das tabelas que precisam ser validadas no dia da execução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034fdcb7",
   "metadata": {},
   "source": [
    "### Aqui é feito um teste para ver se as tabelas tem permissão de acesso.\n",
    "Se tiver alguma tabela que não possui acesso ela será retirada do validador para não quebrar o código e será colocada em uma lista de tabelas sem permissão e no final vai na mensagem informando que não tem acesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_sem_per = []\n",
    "for tb_name in df_tabelas.collect():\n",
    "    try:\n",
    "        spark.sql(f\"select * from {tb_name['nome_tabela']} limit 1\")\n",
    "    except:\n",
    "        tab_sem_per.append(tb_name['nome_tabela'])\n",
    "df_tabelas = df_tabelas.filter(~F.col(\"nome_tabela\").isin(tab_sem_per))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6a295",
   "metadata": {},
   "source": [
    "### Verificando se o job executou ou não\n",
    "Aqui coletamos os dados da tabela através do \"describe history\" pegando a ultima atualização e trazendo dados como data e horário da execução, quantidade de linhas inseridas, nome da tabela e do job.\n",
    "Aqui é feito um loop no dataframe de tabelas para pegar os dados de todas as tabelas que estão para ser validadas no dia da execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = None\n",
    "for table_name in df_tabelas.collect():\n",
    "    history_df = spark.sql(f\"\"\"DESCRIBE HISTORY `{table_name['nome_tabela']}`\"\"\") \\\n",
    "        .filter( #Pegando apenas escrita de tabela e desconsiderando outras operações\n",
    "            (~F.col(\"operation\").isin(\"VACUUM START\", \"VACUUM END\", \"MERGE\"))) \\\n",
    "        .withColumn(\"data_exec\", \n",
    "                    F.to_date(F.col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "        .withColumn(\"hr_exec\", \n",
    "                    F.date_format(\n",
    "                        (F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\") \n",
    "                        - F.expr(\"INTERVAL 3 HOURS\")), \"HH:mm:ss\")) \\\n",
    "        .withColumn(\"nome_tabela_Desc\", \n",
    "                    F.lit(table_name[\"nome_tabela\"]))\n",
    "\n",
    "    # Obtendo apenas a última execução de cada tabela\n",
    "    recent_update_df = history_df.orderBy(F.col(\"timestamp\").desc()).limit(1)\n",
    "\n",
    "    recent_update_df = recent_update_df \\\n",
    "        .withColumn(\"operationMetricsStr\", \n",
    "                    F.to_json(F.col(\"operationMetrics\"))) \\\n",
    "        .withColumn(\"job\", F.to_json(F.col(\"job\")))\n",
    "\n",
    "    #obtendo o nome do job e a quantidade de linhas inseridas\n",
    "    recent_update_df = recent_update_df \\\n",
    "        .withColumn(\"qtd_linhas\", \n",
    "                    F.get_json_object(F.col(\"operationMetricsStr\"), \"$.numOutputRows\").cast(IntegerType())) \\\n",
    "        .withColumn(\"job_name\",\n",
    "                    F.get_json_object(F.col(\"job\"), \"$.jobName\"))\n",
    "\n",
    "    recent_update_df = recent_update_df \\\n",
    "        .select(\n",
    "            \"nome_tabela_Desc\",\n",
    "            \"qtd_linhas\",\n",
    "            \"data_exec\",\n",
    "            \"hr_exec\",            \n",
    "            \"job_name\"\n",
    "        )\n",
    "\n",
    "    if df_final is None: # Concatenando os dados no DataFrame final\n",
    "        df_final = recent_update_df\n",
    "    else:\n",
    "        df_final = df_final.union(recent_update_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60982f7",
   "metadata": {},
   "source": [
    "### Aplicando status final\n",
    "Aqui é criada a coluna \"status\" que vai mostrar se o job deu \"Sucesso\", \"Falha\" ou \"Executou hoje porém não escreveu dado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d8549",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_final = df_tabelas.alias(\"a\") \\\n",
    "    .join(df_final.alias(\"b\"), \n",
    "          on=F.col(\"a.nome_tabela\") == F.col(\"b.nome_tabela_Desc\"), \n",
    "          how=\"left\") \\\n",
    "    .withColumn(\"status\", \n",
    "        F.when((F.col(\"qtd_linhas\") > 0) & (F.col(\"data_exec\") == dt_hj_str), \"Sucesso\") \\\n",
    "        .otherwise(F.lit(\"Falha\"))) \\\n",
    "    .withColumn(\"status\", \n",
    "        F.when(\n",
    "            (F.col(\"qtd_linhas\").isNull()) | (F.col(\"qtd_linhas\") == 0) & \n",
    "            (F.col(\"data_exec\") == dt_hj_str), \"Executou hoje porém não escreveu dado\") \\\n",
    "        .otherwise(F.col(\"status\"))) \\\n",
    "    .withColumn(\"DT_REFE\",\n",
    "        F.lit(dt_hj_str)) \\\n",
    "    .select(\n",
    "        \"nome_tabela\",\n",
    "        \"job_name\",\n",
    "        \"qtd_linhas\",\n",
    "        \"data_exec\",\n",
    "        \"hr_exec\",      \n",
    "        \"status\",\n",
    "        \"DT_REFE\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d65e7",
   "metadata": {},
   "source": [
    "### Obtendo o horário atual e filtrando o dataframe final\n",
    "Como o job roda de 1 em 1 hora, fiz essa validação para ele pegar apenas os job's que rodam da hora que está sendo executado para baixo para não ter perigo de aplicar \"Falha\" a um job que só roda algum tempo depois do horário que está sendo feita a validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_hour = str(spark.sql(\"SELECT hour(current_timestamp()) as current_hour\").collect()[0][0] - 3).zfill(2)\n",
    "hr_1 = f\"{current_hour}:00:00\"\n",
    "join_final = join_final.filter(\n",
    "    F.to_timestamp(\"hr_exec\", \"HH:mm:ss\") <= F.to_timestamp(F.lit(hr_1), \"HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a636d9",
   "metadata": {},
   "source": [
    "### Criando o dataframe de falhas\n",
    "Aqui filtramos o dataframe final para ter apenas \"Falha\" ou \"Executou hoje porém não escreveu dado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_falhas = join_final.filter((F.col(\"status\") == \"Falha\") | (F.col(\"status\") == \"Executou hoje porém não escreveu dado\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ff77c",
   "metadata": {},
   "source": [
    "### Envio da notificação pelo Teams\n",
    "Aqui é feita a verificação se o df_falhas tiver alguma linha enviamos a notificação para o Teams com o dataframe de falhas para que possa ser feita uma ação o quanto antes, caso contrário a notificação informará que não teve falha em job's no horário da verificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64762b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_falhas.count() >= 1:\n",
    "    webhook_url = 'coloque aqui a URL do webhook Teams'\n",
    "\n",
    "    tb_dash_html = df_falhas.toPandas().to_html(index=False) #transformando o dataframe pyspark em html pra enviar via teams\n",
    "    if len(tab_sem_per) > 0:\n",
    "        message = {\n",
    "            \"title\": f\"FALHA DE JOB DAS {current_hour}:00 - Timestamp validado: {exec_timestamp_str}\",\n",
    "            \"text\": f\"ATENÇÃO! ALGUM JOB EXECUTOU ATÉ AS (current_hour):59:00 E EXECUTOU COM FALHA OU NÃO ESCREVEU DADO NA DATA DE HOJE ({dt_hj_str})***<br>\"\n",
    "                    f\"{tb_dash_html}<br>\"\n",
    "                    f\"Essas tabelas sem permissão de acesso conforme abaixo:<br>\"\n",
    "                    f\"{tab_sem_per}<br>\"\n",
    "                    f\"FAVOR PEDIR PARA LIBERAR O ACESSO AS TABELAS<br>\"\n",
    "        }\n",
    "    else:\n",
    "        message = {\n",
    "            \"title\": f\"FALHA DE JOB - Timestamp validado: {exec_timestamp_str}\",\n",
    "            \"text\": f\"ATENÇÃO! ALGUM JOB EXECUTOU ATÉ AS {current_hour}:59:00 E EXECUTOU COM FALHA OU NÃO ESCREVEU DADO NA DATA DE HOJE ({dt_hj_str})***<br>\"\n",
    "                    f\"{tb_dash_html}<br>\"\n",
    "        }\n",
    "\n",
    "    proxies = {\"http\": \"coloque aqui seu proxy\", \"https\": \"coloque aqui seu proxy\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            webhook_url,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            data=json.dumps(message),\n",
    "            timeout=30,\n",
    "            verify=False,\n",
    "            proxies=proxies\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\"Mensagem enviada com sucesso!\")\n",
    "        else:\n",
    "            print(f\"Falha ao enviar mensagem. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao enviar mensagem: {e}\")\n",
    "\n",
    "else:\n",
    "    webhook_url = 'coloque aqui a URL do webhook Teams'\n",
    "\n",
    "    tb_dash_html = df_falhas.toPandas().to_html(index=False)\n",
    "    if len(tab_sem_per) > 0:\n",
    "        message = {\n",
    "            \"title\": f\"VALIDAÇÃO DE JOB'S DAS {current_hour}:00 - Timestamp validado: {exec_timestamp_str}\",\n",
    "            \"text\": f\"*****ATENÇÃO TODOS OS JOB'S QUE RODAM ATÉ AS {current_hour}:59:00 EXECUTARAM COM SUCESSO!!!*****<br>\"\n",
    "                    f\"<br>Existem tabelas sem permissão de acesso conforme abaixo:<br>\"\n",
    "                    f\"{tab_sem_per}<br>\"\n",
    "                    f\"<br>FAVOR PEDIR PARA LIBERAR O ACESSO AS TABELAS<br>*****\"\n",
    "        }\n",
    "    else:\n",
    "        message = {\n",
    "            \"title\": f\"VALIDAÇÃO DE JOB'S DAS {current_hour}:00 - Timestamp validado: {exec_timestamp_str}\",\n",
    "            \"text\": f\"*****ATENÇÃO TODOS OS JOB'S QUE RODAM ATÉ AS {current_hour}:59:00 EXECUTARAM COM SUCESSO!!!*****<br>\"\n",
    "        }\n",
    "\n",
    "    proxies = {\"http\": \"coloque aqui seu proxy\", \"https\": \"coloque aqui seu proxy\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            webhook_url,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            data=json.dumps(message),\n",
    "            timeout=30,\n",
    "            verify=False,\n",
    "            proxies=proxies\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\"Mensagem enviada com sucesso!\")\n",
    "        else:\n",
    "            print(f\"Falha ao enviar mensagem. Status code: {response.status_code}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao enviar mensagem: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
